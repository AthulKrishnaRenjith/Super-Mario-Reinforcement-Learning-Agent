{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv, VecTransposeImage\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "class CustomJoypadSpace(JoypadSpace):\n",
    "    def reset(self, **kwargs):\n",
    "        kwargs.pop('seed', None)\n",
    "        kwargs.pop('options', None)\n",
    "        return super().reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = super().step(action)\n",
    "        return observation, reward, done, info\n",
    "\n",
    "class CustomDummyVecEnv(DummyVecEnv):\n",
    "    def reset(self):\n",
    "        for env_idx in range(self.num_envs):\n",
    "            obs = self.envs[env_idx].reset()\n",
    "            self._save_obs(env_idx, obs)\n",
    "        return self._obs_from_buf()\n",
    "\n",
    "    def step_wait(self):\n",
    "        results = [self.envs[env_idx].step(self.actions[env_idx]) for env_idx in range(self.num_envs)]\n",
    "        for env_idx, (obs, reward, done, info) in enumerate(results):\n",
    "            self.buf_rews[env_idx] = reward\n",
    "            self.buf_dones[env_idx] = done\n",
    "            self.buf_infos[env_idx] = info\n",
    "            if done:\n",
    "                obs = self.envs[env_idx].reset()\n",
    "            self._save_obs(env_idx, obs)\n",
    "        return self._obs_from_buf(), self.buf_rews, self.buf_dones, self.buf_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-v0\")\n",
    "env = CustomJoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "env.render_mode = 'human'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True\n",
    "\n",
    "for step in range(1000): \n",
    "    if done: \n",
    "        env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "\n",
    "env = CustomDummyVecEnv([lambda: env])\n",
    "\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "env = VecTransposeImage(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "log_dir ='./logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=50000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"CnnPolicy\", env, verbose=1, tensorboard_log=log_dir, learning_rate=0.000001, n_steps=512, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(locals_, globals_):\n",
    "    return locals_, globals_, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/PPO_23\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 136 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 3   |\n",
      "|    total_timesteps | 512 |\n",
      "----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 85            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 11            |\n",
      "|    total_timesteps      | 1024          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1684839e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.00417      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 211           |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.000113     |\n",
      "|    value_loss           | 516           |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 76             |\n",
      "|    iterations           | 3              |\n",
      "|    time_elapsed         | 20             |\n",
      "|    total_timesteps      | 1536           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000106623396 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.95          |\n",
      "|    explained_variance   | 0.00495        |\n",
      "|    learning_rate        | 1e-06          |\n",
      "|    loss                 | 52.7           |\n",
      "|    n_updates            | 20             |\n",
      "|    policy_gradient_loss | -0.00106       |\n",
      "|    value_loss           | 103            |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 68            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 29            |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4788386e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | 0.0152        |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 26            |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000495     |\n",
      "|    value_loss           | 64.5          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 7.83602e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.95       |\n",
      "|    explained_variance   | 0.0665      |\n",
      "|    learning_rate        | 1e-06       |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000241   |\n",
      "|    value_loss           | 1.75        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 63           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.446894e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.95        |\n",
      "|    explained_variance   | 0.0819       |\n",
      "|    learning_rate        | 1e-06        |\n",
      "|    loss                 | 0.25         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000355    |\n",
      "|    value_loss           | 1.14         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 62            |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 57            |\n",
      "|    total_timesteps      | 3584          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1204469e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.0165       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.118         |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000138     |\n",
      "|    value_loss           | 0.953         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 61           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.930364e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.95        |\n",
      "|    explained_variance   | 0.00356      |\n",
      "|    learning_rate        | 1e-06        |\n",
      "|    loss                 | 0.0993       |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.000225    |\n",
      "|    value_loss           | 0.59         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 62            |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 74            |\n",
      "|    total_timesteps      | 4608          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3294822e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.00267      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.192         |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000264     |\n",
      "|    value_loss           | 0.67          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 62            |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 81            |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8153223e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.00685      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.17          |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.000246     |\n",
      "|    value_loss           | 0.491         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 62            |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 89            |\n",
      "|    total_timesteps      | 5632          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7397222e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | 0.00486       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.123         |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.000229     |\n",
      "|    value_loss           | 0.365         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 63            |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 96            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2817287e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.00956      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.0707        |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.000226     |\n",
      "|    value_loss           | 0.23          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 63            |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 104           |\n",
      "|    total_timesteps      | 6656          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9710278e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.0113       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.0625        |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.000272     |\n",
      "|    value_loss           | 0.205         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 63            |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 112           |\n",
      "|    total_timesteps      | 7168          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2585428e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.00715      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.12          |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -0.000158     |\n",
      "|    value_loss           | 0.261         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 64           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 119          |\n",
      "|    total_timesteps      | 7680         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.492498e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.95        |\n",
      "|    explained_variance   | 0.012        |\n",
      "|    learning_rate        | 1e-06        |\n",
      "|    loss                 | 0.124        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000113    |\n",
      "|    value_loss           | 0.265        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 64            |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 127           |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0812266e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.00218      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.105         |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.000285     |\n",
      "|    value_loss           | 0.208         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 64            |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 135           |\n",
      "|    total_timesteps      | 8704          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7205325e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | 0.0244        |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 1.55          |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.000122     |\n",
      "|    value_loss           | 2.48          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 64            |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 142           |\n",
      "|    total_timesteps      | 9216          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016050239 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | 0.00152       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 129           |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | 1.8e-05       |\n",
      "|    value_loss           | 310           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 64           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 150          |\n",
      "|    total_timesteps      | 9728         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.534936e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.0502       |\n",
      "|    learning_rate        | 1e-06        |\n",
      "|    loss                 | 0.57         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000183    |\n",
      "|    value_loss           | 15.8         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 64            |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 158           |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5378267e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | -0.0383       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.161         |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.000233     |\n",
      "|    value_loss           | 0.653         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 64           |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 165          |\n",
      "|    total_timesteps      | 10752        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.197824e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.021        |\n",
      "|    learning_rate        | 1e-06        |\n",
      "|    loss                 | 0.122        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000218    |\n",
      "|    value_loss           | 0.573        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 65          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 173         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 3.17347e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.000343    |\n",
      "|    learning_rate        | 1e-06       |\n",
      "|    loss                 | 0.175       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.000293   |\n",
      "|    value_loss           | 0.456       |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 65            |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 180           |\n",
      "|    total_timesteps      | 11776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2298656e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | -0.0076       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.128         |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.000279     |\n",
      "|    value_loss           | 0.399         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 65            |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 188           |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2314646e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | -0.0471       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.136         |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.000139     |\n",
      "|    value_loss           | 0.405         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 65            |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 195           |\n",
      "|    total_timesteps      | 12800         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1005282e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | 0.0477        |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.0932        |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.00026      |\n",
      "|    value_loss           | 0.309         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 65            |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 203           |\n",
      "|    total_timesteps      | 13312         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6494027e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | 0.00304       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.185         |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -4.66e-05     |\n",
      "|    value_loss           | 0.367         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 65           |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 211          |\n",
      "|    total_timesteps      | 13824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.252935e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.0995       |\n",
      "|    learning_rate        | 1e-06        |\n",
      "|    loss                 | 0.174        |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000523    |\n",
      "|    value_loss           | 0.393        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 65            |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 218           |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4917535e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | -0.0124       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.0803        |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.000345     |\n",
      "|    value_loss           | 0.228         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 65           |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 226          |\n",
      "|    total_timesteps      | 14848        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.146939e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | -0.00458     |\n",
      "|    learning_rate        | 1e-06        |\n",
      "|    loss                 | 0.114        |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000378    |\n",
      "|    value_loss           | 0.208        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 65            |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 233           |\n",
      "|    total_timesteps      | 15360         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8006732e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | 0.00499       |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.161         |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.000325     |\n",
      "|    value_loss           | 0.282         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 65            |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 241           |\n",
      "|    total_timesteps      | 15872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1354797e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | -0.00373      |\n",
      "|    learning_rate        | 1e-06         |\n",
      "|    loss                 | 0.325         |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.000637     |\n",
      "|    value_loss           | 0.456         |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\stacked_observations.py:171: UserWarning: VecFrameStack wrapping a VecEnv without terminal_observation info\n",
      "  warnings.warn(\"VecFrameStack wrapping a VecEnv without terminal_observation info\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot step in a done environment! call `reset`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:207\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py:97\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m---> 97\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_frame_stack.py:39\u001b[0m, in \u001b[0;36mVecFrameStack.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     33\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[0;32m     38\u001b[0m ]:\n\u001b[1;32m---> 39\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     observations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacked_obs\u001b[38;5;241m.\u001b[39mupdate(observations, dones, infos)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observations, rewards, dones, infos\n",
      "Cell \u001b[1;32mIn[20], line 27\u001b[0m, in \u001b[0;36mCustomDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 27\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx, (obs, reward, done, info) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx] \u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[20], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 27\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs)]\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx, (obs, reward, done, info) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx] \u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\shimmy\\openai_gym_compatibility.py:116\u001b[0m, in \u001b[0;36mGymV26CompatibilityV0.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\gym\\core.py:314\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 314\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m, in \u001b[0;36mCustomJoypadSpace.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m---> 16\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\nes_py\\wrappers\\joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mTake a step using the given action.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# take the step and record the output\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:17\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m---> 17\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Desktop\\RL_in_gaming\\marioenv\\Lib\\site-packages\\nes_py\\nes_env.py:296\u001b[0m, in \u001b[0;36mNESEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# if the environment is done, raise an error\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot step in a done environment! call `reset`\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# set the action on the controller\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrollers[\u001b[38;5;241m0\u001b[39m][:] \u001b[38;5;241m=\u001b[39m action\n",
      "\u001b[1;31mValueError\u001b[0m: cannot step in a done environment! call `reset`"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('thisisatestmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('C:/Users/ACER/Desktop/RL_in_gaming/thisisatestmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "while True:\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "    if done:\n",
    "        state = env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (marioenv)",
   "language": "python",
   "name": "marioenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
